{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Me\n",
    "\n",
    "Goal is to develop a straightforward application (w/o UI) that accepts a user's question, searches for relevant documents in my web page, feeds both the question and retrieved documents into a language model, and provides an answer. I use langchain for this purpose.\n",
    "\n",
    "**Highlights**:\n",
    "- Langsmith allows to trace basically everything and what is interesting for applications is that it collects data on **latency**, token **usage** and costs per request.\n",
    "- It also breaks down the request into its constitutive parts including the augmented prompt. This meta data can probably help to optimize the splits and other parameters. I don't know yet limits of langsmith api \n",
    "- The usual roadblock for application is there, i.e. privacy and security. It'd be cool to try a local model, eg the new student llama, for the inference part\n",
    "\n",
    "#### Setup the environment and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "print(ssl.OPENSSL_VERSION)\n",
    "# libressl 2.6.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🚫 under the current env I have LibraSSL and not OpenSSL. This leads to api connection error according to [openai](https://help.openai.com/en/articles/6897191-apiconnectionerror). I can install openSSL and recompile python but prefer to use Conda since python distribution is precompiled there. So I create the env with conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenSSL 3.3.2 3 Sep 2024\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "print(ssl.OPENSSL_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was about to use Anthropic LLM but counldn't find an embedding model for it. It seems like langchain still uses `OpenAIEmbeddings` no matter what LLM is used. So if I have to anyway pay for the openai api, I'd rather use it for both LLM and embeddings. Let's check them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# set the openai api key from my environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.016751619055867195, -0.055799614638090134, 0.005647437181323767]\n"
     ]
    }
   ],
   "source": [
    "# check if ssl version is okay, try an openai embeddings\n",
    "from langchain_openai import OpenAIEmbeddings  \n",
    "\n",
    "embed = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "input_text = \"The meaning of life is 42\"  \n",
    "vector = embed.embed_query(\"hello\")  \n",
    "print(vector[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f59a81427f', 'finish_reason': 'stop', 'logprobs': None}, id='run-c8b125fa-2fd0-43ee-b09d-a517078b2fad-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test of the chat model, use gpt-4o-mini which is cost efficient\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both are working. Let's setup the langchain and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable LangChain tracing, curious to see how it works\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# set the langchain api key from my environment variable\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the issue with USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
    "os.environ['USER_AGENT'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🚫 Chroma has a compatibility issue with python version (3.13), so I head back to 3.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's begin loading the content\n",
    "\n",
    "Langchain has an infinite number of [loaders](https://python.langchain.com/docs/integrations/document_loaders/). There are only a few types (eg web, json, pdf, etc) but per type you get an endless number of loaders. I'll start with the web loader. Let's load the content of my page. \n",
    "It should return the page as lists of strings and metadata for each string.\n",
    "\n",
    "I'd like to include also the subpages, eg my Event page, I can add it as a separate link and load it but there should be a smarter way to do that. For now I stick to the main page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters in the content:  1732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'here are all of the content:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nAmir Khalilzadeh\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\nEvents\\n\\n\\n\\n\\n \\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAmir Khalilzadeh\\n\\n\\n\\n\\n\\n\\n\\nLinkedIn\\n\\n\\n\\nGithub\\n\\n\\n Email\\n\\n\\n\\n\\n\\n\\nabout me\\nI am a product lead at the Continuing Education of EPFL and UNIL. I co-lead and teach a COS program, Applied Data Science and Machine Learning, to private individuals and industry professionals. I also (co)-deliver on-demand workshops to companies and organizations.\\nI spend most of my day helping our learners understand the underlying principles of data pipelines and ML models, identify issues in their Python code, and guide them in framing their ML projects with data that they bring from their work or else where.\\n\\n\\nEvent updates 🤖🛠️\\n\\n2025/05 | a short course on LLMs is under construction 🚧 🤩\\n2024/10 | delivered a 2-days workshop at Logitech\\n2024/09 | delivered an NLP masterclass\\n\\n2024/03 | delivered a half-day hands-on workshop at AMLD\\n2023/09 | delivered a 2-days hands-on workshop at the World Economic Forum\\n2023/01 | delivered a full day workshop to Nestle\\n2022/06 | delivered a full day workshop to World Trade Organization\\n2021/10 | delivered a full day workshop to Total Energies\\n2021/01 | we just lunched “That’s AI” platform for AI education 🎉\\n2021/11 | we released a new version of our popular ADSML program after a year of development 💪 🥳\\n\\n\\n\\nCareer updates 🔔\\n\\n2024/08 | now a Product Lead at the Continuing Education of EPFL and UNIL\\n2023/08 | I am promoted to senior course developer and instructor at Extension School\\n2021/01 | Extension School is now part of the Continuing Education of EPFL.\\n2020/08 | I started a new role at an Edtech startup, Extension School, as a course developer and instructor.\\n2018/09 | I started my postdoc at EPFL\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is the metadata:\n",
      " {'source': 'https://amirkhalilzadeh.github.io/wp/', 'title': 'Amir Khalilzadeh', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "# lets get the contents of my page\n",
    "\n",
    "loader = WebBaseLoader(web_paths=(\"https://amirkhalilzadeh.github.io/wp/\",),)\n",
    "docs = loader.load()\n",
    "\n",
    "# lets see the content and metadata \n",
    "print('number of characters in the content: ', len(docs[0].page_content))\n",
    "display('here are all of the content:', docs[0].page_content)\n",
    "print('here is the metadata:\\n', docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the content\n",
    "\n",
    "GPT-4o mini has 128k context window, so my page with only 1.7k characters doesn't even scratch the surface and splitting is not really necessary. But I'll do it anyway to see how it works and I guess it will help the embeddings to be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://amirkhalilzadeh.github.io/wp/', 'title': 'Amir Khalilzadeh', 'language': 'en'}, page_content='Amir Khalilzadeh\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\nEvents')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chunk the contents of the page\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20, add_start_index=False)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "display(len(splits))\n",
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1753"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a for loop to get the length of each split and check disribution of the counts\n",
    "lengths = [len(split.page_content) for split in splits]\n",
    "sum(lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum is slightly more than the total number of characters.\n",
    "\n",
    "#### Store the embeddings\n",
    "\n",
    "It's now time to store the information in the database and that essentially means to store the embeddings. I'll go for `text-embedding-3-small` because it is the [cheapest](https://openai.com/api/pricing/). The vector length is 1536 (vs 3072 for the large version). The price-performance of small vs large is great, see [here](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings), and more on the openai embeddings [here](https://openai.com/index/new-embedding-models-and-api-updates/) from earlier this January."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x110de23e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the chunks in a vector database\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve information\n",
    "\n",
    "By default, the vector store retriever uses similarity search, and it is possible to set the number of results to return. I started with 1 but it clearly failed to find similar content for the following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'language': 'en', 'source': 'https://amirkhalilzadeh.github.io/wp/', 'title': 'Amir Khalilzadeh'}, page_content='about me'),\n",
       " Document(metadata={'language': 'en', 'source': 'https://amirkhalilzadeh.github.io/wp/', 'title': 'Amir Khalilzadeh'}, page_content='developer and instructor.'),\n",
       " Document(metadata={'language': 'en', 'source': 'https://amirkhalilzadeh.github.io/wp/', 'title': 'Amir Khalilzadeh'}, page_content='Amir Khalilzadeh\\n\\n\\n\\n\\n\\n\\n\\nLinkedIn\\n\\n\\n\\nGithub\\n\\n\\n Email'),\n",
       " Document(metadata={'language': 'en', 'source': 'https://amirkhalilzadeh.github.io/wp/', 'title': 'Amir Khalilzadeh'}, page_content='Amir Khalilzadeh\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\nEvents'),\n",
       " Document(metadata={'language': 'en', 'source': 'https://amirkhalilzadeh.github.io/wp/', 'title': 'Amir Khalilzadeh'}, page_content='them in framing their ML projects with data that they bring from their work or else where.'),\n",
       " Document(metadata={'language': 'en', 'source': 'https://amirkhalilzadeh.github.io/wp/', 'title': 'Amir Khalilzadeh'}, page_content='2024/08 | now a Product Lead at the Continuing Education of EPFL and UNIL')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve similar information from the vector database\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "# lets see how the retriever works for a sample query\n",
    "out = retriever.invoke(\"who is the author of the page?\")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment the input query\n",
    "\n",
    "Langchain has a hub for prompts, eg `rag-prompt` (see below) but I'll use a custom one.\n",
    "\n",
    "*You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.*\n",
    "\n",
    "Let's chain everything together and see if it works. The **prompt** is fixed, and **context** is given by the retriever, and **question** is the user input. The **llm** takes the questions which is augmented with the context and prompt, yet they fit into its context window. It has all the information it needs to generate an answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructs a prompt \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, check the [page](https://amirkhalilzadeh.github.io/wp/) yourself but it's likely that there is no information about your question.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain things together\n",
    "\n",
    "Let's chain everything together and see if it works. The prompt is fixed, and context is given by the retriever, and question is the user input. The llm takes the questions which is augmented with the context and prompt, yet they fit into its context window. It has all the information it needs to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# passes things to the llm and parses the output\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The author of the page is Amir Khalilzadeh.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"who is the author of the page?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He has delivered a total of six workshops so far.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"how many workshops he has delivered so far?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, there is a short course on LLMs under construction for May 2025.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"does he have any workshop planned for next year, 2025?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The audience for his most recent workshop, delivered in October 2024, was likely professionals from Logitech.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"who was the audience for his most recent workshop?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lot's things to do to improve the QA chat and make it more practical but for now I stop here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
